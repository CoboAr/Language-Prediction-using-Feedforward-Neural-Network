{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "2fkg6DLf6skQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gPnkqrsAvOUX"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize, ngrams\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "from google.colab import files\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross-Entropy Loss Function"
      ],
      "metadata": {
        "id": "cIMKRqgG7MHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.tensor([1]).long()\n",
        "print(target)\n",
        "predicted = torch.tensor( [-2.5, -4.212] ).float()\n",
        "print(predicted.view(1,2))\n",
        "lossfxn = nn.CrossEntropyLoss()\n",
        "loss = lossfxn(predicted.view(1,2), target)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3F_dGZi7NIa",
        "outputId": "0d6a0b57-eefd-426c-948a-0cbc9212a835"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n",
            "tensor([[-2.5000, -4.2120]])\n",
            "tensor(1.8779)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Files for Training"
      ],
      "metadata": {
        "id": "gqM7r7EF-orV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findFiles():\n",
        "    file1 = files.upload()\n",
        "    file2 = files.upload()\n",
        "    return file1, file2\n",
        "\n",
        "files = findFiles()\n",
        "print(files)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cB0IjmoL-taL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Feed-Forward Neural Network Model"
      ],
      "metadata": {
        "id": "KRCWfYdj7PMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Feedforward Neural Network\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Non-linearity\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        out = self.sigmoid(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rfswrp8H7Suv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess Text Data"
      ],
      "metadata": {
        "id": "SuKbUcbDRR3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: Convert Unicode to ASCII and build a category dictionary\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" \\'\\\"_.,;-'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn' and c in all_letters\n",
        "    )\n",
        "\n",
        "print(unicodeToAscii('Ślusàrski'))\n",
        "print(unicodeToAscii('Ço\"b\"_\\'o\\' Është'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQkKILPKRKZD",
        "outputId": "add535f6-bf1d-4052-a12f-533325bc5e0e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slusarski\n",
            "Co\"b\"_'o' Eshte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data and Build Bigram Tensors"
      ],
      "metadata": {
        "id": "h4CccJ1HSxWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the category_lines dictionary, a list of words per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "# Read a file and split into lines\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "AllLines = \"\"\n",
        "for file in files:\n",
        "    for filename, content in file.items():\n",
        "        category = os.path.splitext(os.path.basename(filename))[0]\n",
        "        all_categories.append(category)\n",
        "        lines = readLines(filename)\n",
        "        category_lines[category] = lines\n",
        "        AllLines = AllLines + \" \" + ' '.join(lines)\n",
        "\n",
        "n_categories = len(all_categories)"
      ],
      "metadata": {
        "id": "mwFRzUCgSxpo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bigram Functions"
      ],
      "metadata": {
        "id": "WxdSnYK5TE3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract bigrams from text content\n",
        "def findAllBigrams(content):\n",
        "    bigrams = list(ngrams(content, 2, pad_right=True, right_pad_symbol=' '))\n",
        "    noDuplicate = list(dict.fromkeys(bigrams))\n",
        "    return noDuplicate\n",
        "\n",
        "# Extract bigrams from the full content\n",
        "all_bigrams = findAllBigrams(AllLines.lower())\n",
        "\n",
        "# Initialize the feedforward neural network model with bigram size input\n",
        "FDM = FeedforwardNeuralNetModel(len(all_bigrams), 10, 2)"
      ],
      "metadata": {
        "id": "yBalc6W-TFSh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bigram-to-Index and Tensor Conversion"
      ],
      "metadata": {
        "id": "YJDGrmvoTUmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the index of a bigram\n",
        "def bigramToIndex(bigram):\n",
        "    if bigram in all_bigrams:\n",
        "        return all_bigrams.index(bigram)\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Convert a bigram to a tensor\n",
        "def bigramToTensor(bigram):\n",
        "    tensor = torch.zeros(1, len(all_bigrams))\n",
        "    n = bigramToIndex(bigram)\n",
        "    if n > -1:\n",
        "        tensor[0][n] = 1\n",
        "    return tensor\n",
        "\n",
        "# Convert a sentence (line) to a tensor of bigrams\n",
        "def lineToTensor(line):\n",
        "    Linebigrams = list(ngrams(line, 2, pad_right=True, right_pad_symbol=' '))\n",
        "    tensor = torch.zeros(len(all_bigrams))\n",
        "    for i in range(len(Linebigrams)):\n",
        "        n = bigramToIndex(Linebigrams[i])\n",
        "        if n > -1:\n",
        "            tensor[n] += 1\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "4oOZIP0zTU4P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Training Generator"
      ],
      "metadata": {
        "id": "BIbTXcvlTnZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to randomly select a training example\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "# Function to randomly select a category and a sentence for training\n",
        "def randomTrainingExample():\n",
        "    category = randomChoice(all_categories)\n",
        "    line = randomChoice(category_lines[category])\n",
        "    category_tensor = torch.tensor([all_categories.index(category)]).long()\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return category, line, category_tensor, line_tensor\n",
        "\n",
        "# Print random examples\n",
        "for i in range(2):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    print('category =', category, '/ line =', line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H2XxdM8Toa_",
        "outputId": "2884410c-fefd-4228-ac9d-566cf870359f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "category = English / line = The ancient tapestry hung in the grand hall, depicting battles fought long ago.\n",
            "category = Albanian / line = Nje studente nga Universiteti i Shkodres fitoi nje cmim te rendesishem per projektin e saj mbi zhvillimin e turizmit te qendrueshem ne Shqiperi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Neural Network"
      ],
      "metadata": {
        "id": "eoWff4cFUOt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Entropy Loss Function"
      ],
      "metadata": {
        "id": "tnykG4rytNez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr = 0.01\n",
        "\n",
        "# Cross-Entropy Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(FDM.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "    optimizer.zero_grad()  # Clears existing gradients\n",
        "    output = FDM(line_tensor)  # Forward pass\n",
        "    loss = criterion(output.view(1, 2), category_tensor)  # Calculate loss\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETG18oLzUO_8",
        "outputId": "8741d868-543b-4a76-c2f4-3a99705baea1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100............. Loss: 0.5551\n",
            "Epoch: 20/100............. Loss: 0.7083\n",
            "Epoch: 30/100............. Loss: 0.5773\n",
            "Epoch: 40/100............. Loss: 0.7346\n",
            "Epoch: 50/100............. Loss: 0.5115\n",
            "Epoch: 60/100............. Loss: 0.7018\n",
            "Epoch: 70/100............. Loss: 0.5053\n",
            "Epoch: 80/100............. Loss: 0.6896\n",
            "Epoch: 90/100............. Loss: 0.6587\n",
            "Epoch: 100/100............. Loss: 0.6438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BCEWithLogitsLoss Loss Function"
      ],
      "metadata": {
        "id": "oK_zL-bCtPxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr = 0.15\n",
        "\n",
        "# BCEWithLogitsLoss Loss Function\n",
        "criterion = nn.BCEWithLogitsLoss()  # Replacing CrossEntropyLoss\n",
        "optimizer = torch.optim.SGD(FDM.parameters(), lr=lr)\n",
        "\n",
        "# Function to create one-hot encoded target tensor\n",
        "def create_one_hot_target(category_index, num_classes=2):\n",
        "    target = torch.zeros(num_classes)\n",
        "    target[category_index] = 1  # One-hot encode the target\n",
        "    return target\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "\n",
        "    # Convert the category_tensor to one-hot encoding\n",
        "    category_tensor = create_one_hot_target(all_categories.index(category), 2).float()\n",
        "\n",
        "    optimizer.zero_grad()  # Clears existing gradients\n",
        "    output = FDM(line_tensor)  # Forward pass\n",
        "\n",
        "    # Match the target tensor size with the output tensor size\n",
        "    loss = criterion(output.view(1, -1), category_tensor.view(1, -1))  # Use BCEWithLogitsLoss\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}/{n_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noskmTJybjeh",
        "outputId": "e445f302-1972-4925-a49d-2c4b723871e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100, Loss: 0.8916890025138855\n",
            "Epoch: 20/100, Loss: 0.7226808071136475\n",
            "Epoch: 30/100, Loss: 0.7862013578414917\n",
            "Epoch: 40/100, Loss: 0.28589195013046265\n",
            "Epoch: 50/100, Loss: 0.5265478491783142\n",
            "Epoch: 60/100, Loss: 0.2409517467021942\n",
            "Epoch: 70/100, Loss: 0.2111690789461136\n",
            "Epoch: 80/100, Loss: 0.17957395315170288\n",
            "Epoch: 90/100, Loss: 0.132507786154747\n",
            "Epoch: 100/100, Loss: 0.12470125406980515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KLDivLoss Loss Function"
      ],
      "metadata": {
        "id": "BrjDJhmitRWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr = 0.01\n",
        "\n",
        "# KLDivLoss Loss Function\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')  # 'batchmean' gives the correct KL divergence per batch\n",
        "optimizer = torch.optim.SGD(FDM.parameters(), lr=0.01)\n",
        "\n",
        "# Function to create one-hot encoded probability target tensor\n",
        "def create_prob_target(category_index, num_classes=2):\n",
        "    target = torch.zeros(num_classes)\n",
        "    target[category_index] = 1  # One-hot encode the target\n",
        "    target = target / target.sum()  # Ensure the target is a probability distribution\n",
        "    return target\n",
        "\n",
        "# Training loop for KLDivLoss\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "\n",
        "    # Convert the category_tensor to one-hot encoding as a probability distribution\n",
        "    category_tensor = create_prob_target(all_categories.index(category), 2).float()\n",
        "\n",
        "    optimizer.zero_grad()  # Clears existing gradients\n",
        "    output = FDM(line_tensor)  # Forward pass\n",
        "\n",
        "    # Apply log_softmax to the output to convert it into log probabilities\n",
        "    log_prob_output = torch.log_softmax(output.view(1, -1), dim=1)\n",
        "\n",
        "    # Match the target tensor size with the output tensor size\n",
        "    loss = criterion(log_prob_output, category_tensor.view(1, -1))  # Use KLDivLoss\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}/{n_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVjZEqvtxfON",
        "outputId": "5bf4daed-d6c2-4ce0-c7e4-c54b757e9f61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100, Loss: 0.8777222633361816\n",
            "Epoch: 20/100, Loss: 0.7585684061050415\n",
            "Epoch: 30/100, Loss: 0.5282343626022339\n",
            "Epoch: 40/100, Loss: 0.6515924334526062\n",
            "Epoch: 50/100, Loss: 0.601322591304779\n",
            "Epoch: 60/100, Loss: 0.5922287106513977\n",
            "Epoch: 70/100, Loss: 0.6694270968437195\n",
            "Epoch: 80/100, Loss: 0.6262274384498596\n",
            "Epoch: 90/100, Loss: 0.580669105052948\n",
            "Epoch: 100/100, Loss: 0.4838118553161621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SoftMarginLoss Loss Function"
      ],
      "metadata": {
        "id": "YVt_51kZVxHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr = 0.001\n",
        "\n",
        "# SoftMarginLoss Loss Function\n",
        "criterion = nn.MultiLabelSoftMarginLoss()  # Use MultiLabelSoftMarginLoss for multi-class tasks\n",
        "optimizer = torch.optim.SGD(FDM.parameters(), lr=0.001)\n",
        "\n",
        "# Function to convert category index to -1 and 1 for SoftMarginLoss\n",
        "def create_binary_target(category_index, num_classes=2):\n",
        "    target = torch.full((num_classes,), -1)  # Initialize with -1 for all classes\n",
        "    target[category_index] = 1  # Set the correct class to 1\n",
        "    return target\n",
        "\n",
        "# Training loop for SoftMarginLoss\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
        "\n",
        "    # Convert the category_tensor to -1 and 1 encoding\n",
        "    category_tensor = create_binary_target(all_categories.index(category), 2).float()\n",
        "\n",
        "    # # Print target tensor to verify it\n",
        "    # print(f'Epoch: {epoch}, Target tensor: {category_tensor}')\n",
        "\n",
        "    optimizer.zero_grad()  # Clears existing gradients\n",
        "    output = FDM(line_tensor)  # Forward pass\n",
        "\n",
        "    # Match the target tensor size with the output tensor size\n",
        "    loss = criterion(output.view(1, -1), category_tensor.view(1, -1))  # Use SoftMarginLoss\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}/{n_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qTT2-NlV0uK",
        "outputId": "cbaee631-6ef1-430e-c550-574c2cf8ec23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100, Loss: 0.6314465403556824\n",
            "Epoch: 20/100, Loss: 0.29672446846961975\n",
            "Epoch: 30/100, Loss: 0.5980336666107178\n",
            "Epoch: 40/100, Loss: 0.5915906429290771\n",
            "Epoch: 50/100, Loss: 0.5747272968292236\n",
            "Epoch: 60/100, Loss: 0.5958119630813599\n",
            "Epoch: 70/100, Loss: 0.49480384588241577\n",
            "Epoch: 80/100, Loss: 0.27307549118995667\n",
            "Epoch: 90/100, Loss: 0.5221474766731262\n",
            "Epoch: 100/100, Loss: 0.2614022195339203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction Function"
      ],
      "metadata": {
        "id": "oip8iAcbUSDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the language of a new sentence\n",
        "def predict(input_line, n_predictions=2):\n",
        "    print('\\n> %s' % input_line)\n",
        "    line_tensor = lineToTensor(input_line)\n",
        "    with torch.no_grad():\n",
        "        output = FDM(line_tensor)  # Forward pass for prediction\n",
        "        print(\"Output is:\")\n",
        "        print(output)\n",
        "        predictions = []\n",
        "        result = torch.argmax(output)\n",
        "        category_index = result.item()\n",
        "        print(\"Predicted Language:\", all_categories[category_index])\n",
        "\n",
        "# Test predictions\n",
        "print(\"The prediction for 10 English sentences: \")\n",
        "predict('The sun dipped below the horizon, painting the sky in shades of pink and orange.')\n",
        "predict('Olivia laughed as the puppy chased its tail in endless circles.')\n",
        "predict('The library was quiet except for the soft rustling of turning pages.')\n",
        "predict('A gentle breeze carried the scent of flowers through the open window.')\n",
        "predict('The old clock tower chimed, signaling the start of a new day.')\n",
        "predict('Jacob wrote a letter to his grandmother, telling her about his adventures.')\n",
        "predict('The waves crashed against the shore, leaving seashells scattered along the beach.')\n",
        "predict('Sarah carefully folded the paper crane and added it to her growing collection.')\n",
        "predict('The hikers reached the summit, rewarded with a breathtaking view of the valley below.')\n",
        "predict('A single raindrop hit the window, followed by a sudden downpour.')\n",
        "print(\"\\n*******************************************************************************\")\n",
        "print(\"*******************************************************************************\")\n",
        "print(\"\\nThe prediction for 10 Albanian sentences: \")\n",
        "predict('Dielli u zhyt nën horizont, duke ngjyrosur qiellin me nuanca rozë dhe portokalli.')\n",
        "predict('Olivia qeshi ndërsa këlyshi ndiqte bishtin e tij në qarqe të pafundme.')\n",
        "predict('Biblioteka ishte e qetë përveç fërshëllimës së lehtë të faqeve që ktheheshin.')\n",
        "predict('Një fllad i lehtë solli aromën e luleve përmes dritares së hapur.')\n",
        "predict('Kulla e vjetër e orës ra, duke shënuar fillimin e një dite të re.')\n",
        "predict('Jakobi i shkroi një letër gjyshes së tij, duke i treguar për aventurat e tij.')\n",
        "predict('Valët përplaseshin në breg, duke lënë guaska të shpërndara përgjatë plazhit.')\n",
        "predict('Sara palosi me kujdes vinçin prej letre dhe e shtoi në koleksionin e saj që rritej.')\n",
        "predict('Alpinistët arritën në majë, të shpërblyer me një pamje mahnitëse të luginës poshtë.')\n",
        "predict('Një pikë e vetme shiu goditi dritaren, e ndjekur nga një rrebesh papritur.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIpV9WCdUSVM",
        "outputId": "8e789ee6-c4a3-464e-da07-e86d6fa7d883"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction for 10 English sentences: \n",
            "\n",
            "> The sun dipped below the horizon, painting the sky in shades of pink and orange.\n",
            "Output is:\n",
            "tensor([0.1827, 0.3853])\n",
            "Predicted Language: English\n",
            "\n",
            "> Olivia laughed as the puppy chased its tail in endless circles.\n",
            "Output is:\n",
            "tensor([0.2075, 0.3890])\n",
            "Predicted Language: English\n",
            "\n",
            "> The library was quiet except for the soft rustling of turning pages.\n",
            "Output is:\n",
            "tensor([0.2078, 0.3881])\n",
            "Predicted Language: English\n",
            "\n",
            "> A gentle breeze carried the scent of flowers through the open window.\n",
            "Output is:\n",
            "tensor([0.2179, 0.3426])\n",
            "Predicted Language: English\n",
            "\n",
            "> The old clock tower chimed, signaling the start of a new day.\n",
            "Output is:\n",
            "tensor([0.1978, 0.3899])\n",
            "Predicted Language: English\n",
            "\n",
            "> Jacob wrote a letter to his grandmother, telling her about his adventures.\n",
            "Output is:\n",
            "tensor([0.2689, 0.3645])\n",
            "Predicted Language: English\n",
            "\n",
            "> The waves crashed against the shore, leaving seashells scattered along the beach.\n",
            "Output is:\n",
            "tensor([0.1803, 0.4453])\n",
            "Predicted Language: English\n",
            "\n",
            "> Sarah carefully folded the paper crane and added it to her growing collection.\n",
            "Output is:\n",
            "tensor([0.2067, 0.3366])\n",
            "Predicted Language: English\n",
            "\n",
            "> The hikers reached the summit, rewarded with a breathtaking view of the valley below.\n",
            "Output is:\n",
            "tensor([0.1731, 0.4248])\n",
            "Predicted Language: English\n",
            "\n",
            "> A single raindrop hit the window, followed by a sudden downpour.\n",
            "Output is:\n",
            "tensor([0.1849, 0.3897])\n",
            "Predicted Language: English\n",
            "\n",
            "*******************************************************************************\n",
            "*******************************************************************************\n",
            "\n",
            "The prediction for 10 Albanian sentences: \n",
            "\n",
            "> Dielli u zhyt nën horizont, duke ngjyrosur qiellin me nuanca rozë dhe portokalli.\n",
            "Output is:\n",
            "tensor([0.3075, 0.1904])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Olivia qeshi ndërsa këlyshi ndiqte bishtin e tij në qarqe të pafundme.\n",
            "Output is:\n",
            "tensor([0.3938, 0.1393])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Biblioteka ishte e qetë përveç fërshëllimës së lehtë të faqeve që ktheheshin.\n",
            "Output is:\n",
            "tensor([0.3148, 0.2399])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Një fllad i lehtë solli aromën e luleve përmes dritares së hapur.\n",
            "Output is:\n",
            "tensor([0.2847, 0.2718])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Kulla e vjetër e orës ra, duke shënuar fillimin e një dite të re.\n",
            "Output is:\n",
            "tensor([0.3947, 0.1404])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Jakobi i shkroi një letër gjyshes së tij, duke i treguar për aventurat e tij.\n",
            "Output is:\n",
            "tensor([0.3311, 0.1666])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Valët përplaseshin në breg, duke lënë guaska të shpërndara përgjatë plazhit.\n",
            "Output is:\n",
            "tensor([0.2985, 0.2535])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Sara palosi me kujdes vinçin prej letre dhe e shtoi në koleksionin e saj që rritej.\n",
            "Output is:\n",
            "tensor([0.3613, 0.1009])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Alpinistët arritën në majë, të shpërblyer me një pamje mahnitëse të luginës poshtë.\n",
            "Output is:\n",
            "tensor([0.3354, 0.1616])\n",
            "Predicted Language: Albanian\n",
            "\n",
            "> Një pikë e vetme shiu goditi dritaren, e ndjekur nga një rrebesh papritur.\n",
            "Output is:\n",
            "tensor([0.3901, 0.0753])\n",
            "Predicted Language: Albanian\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvlUI26Ej6RV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}